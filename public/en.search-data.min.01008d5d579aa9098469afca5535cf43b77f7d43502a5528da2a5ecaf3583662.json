[{"id":0,"href":"/docs/blog-down/blog-down/","title":"Using R Blogdown for Site Building","section":"Docs","content":"Using R Blogdown for Site Building #  It\u0026rsquo;s such a learning experience in setting up a website using github+ terminal + R blogdown + netlify for me. In general, I followed the post from [Alison Hill] (https://alison.rbind.io/post/up-and-running-with-blogdown/).\nA couple of problems have occured and I have searched very hard to resolve them.\n   R crash whenever I tried to open projects in Mac OS system.    Solution: download update R version from (https://www.rstudio.com/products/rstudio/download/). Now I have Rstudio 1.1.463 - Mac OS X in my mac.\n  I don\u0026rsquo;t know how to push the local materials to github.    Solution: It seems there are several ways to push local files to github. In R I follow the path Tools \u0026gt; Version Control \u0026gt; git. Then I select the folders that I would like to upload to github. However, some of the folders were NOT staged. In another word, I could not select these Not staged files in order to commit and push them to github.\nTo go around this, I used terminal. In terminal, I used cd to go to the local github clone file. Then I used command git add -A . All files in the folders were able to upload to github then.\n  I could not publish my sites in netlify.    After uploading all neccessary needed files into github, especially the \u0026lsquo;public\u0026rsquo; folder, I followed the deploy setting but failed. The key step here turn out to be that you need to make sure that the hugo versions are consistent in your machine and netlify.\nIn terminal, I typed in `` Hugo versoins` to obtain the hugo versions 0.51 in my machine. In netlify, I added the variables in the deploy setting, Hugo_version, and set the value of 0.51.\n  Create a new post option 1: use addins \u0026lt; NewPost\u0026gt; which will create a newpost under the folder Content/Post option 2: create a Post folder under Content/Post    Insert an image in RMD file and show it in blogdown    option 1: use addins during the edits of .md file This will generate a code in the file for example\n![](/post/2019-03-27-median-survival-time_files/survivalpost1.png) The file directory is under static/ with the relative directory path\noption 2: put the file in the corresponding post file under static/ , and enter the file relative file path. For example, here is another code to add in a image which will allow image size adjustment\nknitr::include_graphics(\u0026quot;/post/2019-03-27-median-survival-time_files/survivalpost2.png\u0026quot;)   use git shell language to push changes Reference: [link] (https://help.github.com/en/articles/adding-an-existing-project-to-github-using-the-command-line)    I did three lines of commands git add . git commit -m \u0026ldquo;change\u0026rdquo; git push -u origin master\n  I was trying to change my themes from academic to a more simple documentation theme. Instead of doing install_theme () which will create many errors during the installation. You may follow the direction https://geekdocs.de/usage/getting-started/ . step1: you may first create a new site in the git clone folder, using new_site () in R, which will use a default theme. step2: install the geekdoc theme in the site folder. You should see geekdoc folder appear in theme folder within the site folder. step 3: you may just change the config.toml to the theme of geekdoc. Then run serve_site().    "},{"id":1,"href":"/docs/psm/","title":"Propensity score methods","section":"Docs","content":"\n\n\n\n"},{"id":2,"href":"/docs/plot/sankey/sankey-020722/","title":"Sankey diagram (R)","section":"Docs","content":" Sankey diagram A Sankey Diagram is a popular data visualization for data flows display. For example, hospitals from better, average, worse performance categories can fall into different performance categories after quality improvement efforts. A Sankey diagram can be used to display such a change in categories. The categorical buckets (nodes) are represented by boxes or text at each column in the plot. The transition across nodes are connected by links. These links have a width proportional to the weight of the flow, which could be percentage the or number of subjects.\nThe R package (networkD3) can be used to create a sankey diagram. The key is to construct two dataset: 1) nodes: coding each bucket at different column in a sequential order, a group variable can be added; 2) links: the connections across different nodes. A detailed reference and examples for the R package can be reviewed here: https://www.data-to-viz.com/graph/sankey.html\nA data example: The goal of the following codes are to describe the change of locations after a left ventricular assist device (LVAD) implant (day 0) at day 90, 180 and 365. These possible locations such as home, SNF, hospitals, death, etc are coded as 1 to 9. Given the sensitivity of the data, the following codes may not be comprehensible without the data context.\n#the nodes data can be managed manually, the key is that the first entry would be recognized as 0 in the sankey diagram function, thus the coding order has to be started with 0; nodes\u0026lt;- read.csv (\u0026quot;.../nodes2.csv\u0026quot;) dat\u0026lt;- read.csv (\u0026quot;../final_location_4dates_w.csv\u0026quot;) library(tidyverse) dat$cat_location_d0_re=0 dat$cat_location_d90 [is.na(dat$cat_location_d90)]=9 # coding the NA subject as censor; dat$cat_location_d180 [is.na(dat$cat_location_d180)]=9 # coding the NA subject as censor; dat$cat_location_d365 [is.na(dat$cat_location_d365)]=9 # coding the NA subject as censor; # recode the locations at day 180 and day 365 to the sequential coding (day 90 is already set); dat11\u0026lt;- dat %\u0026gt;% mutate(cat_location_d180_re = recode(cat_location_d180, \u0026quot;1\u0026quot;=\u0026quot;10\u0026quot;,\u0026quot;2\u0026quot;=\u0026quot;11\u0026quot;, \u0026quot;3\u0026quot;=\u0026quot;12\u0026quot;,\u0026quot;4\u0026quot;=\u0026quot;13\u0026quot;,\u0026quot;5\u0026quot;=\u0026quot;14\u0026quot;, \u0026quot;6\u0026quot;=\u0026quot;15\u0026quot;, \u0026quot;7\u0026quot;=\u0026quot;16\u0026quot;,\u0026quot;8\u0026quot;=\u0026quot;17\u0026quot;, \u0026quot;9\u0026quot;=\u0026quot;18\u0026quot;)) dat12 \u0026lt;- dat11 %\u0026gt;% mutate(cat_location_d365_re = recode(cat_location_d365, \u0026quot;1\u0026quot;=\u0026quot;19\u0026quot;,\u0026quot;2\u0026quot;=\u0026quot;20\u0026quot;, \u0026quot;3\u0026quot;=\u0026quot;21\u0026quot;,\u0026quot;4\u0026quot;=\u0026quot;22\u0026quot;,\u0026quot;5\u0026quot;=\u0026quot;23\u0026quot;, \u0026quot;6\u0026quot;=\u0026quot;24\u0026quot;, \u0026quot;7\u0026quot;=\u0026quot;25\u0026quot;,\u0026quot;8\u0026quot;=\u0026quot;26\u0026quot;, \u0026quot;9\u0026quot;=\u0026quot;27\u0026quot;)) # count the weight of each transition from day 0 (d0) to day 90 (d90); dat1\u0026lt;- data.frame(dplyr::count_(dat12, vars = c(\u0026#39;cat_location_d0_re\u0026#39;,\u0026#39;cat_location_d90\u0026#39;))) # rename the variables; dat1 \u0026lt;- dat1 %\u0026gt;% rename(source=cat_location_d0_re, target=cat_location_d90 , weight=n) # count the weight of each transition from day 90 to day 180; dat2\u0026lt;- data.frame(dplyr::count_(dat12, vars = c(\u0026#39;cat_location_d90\u0026#39;,\u0026#39;cat_location_d180_re\u0026#39;))) dat2 \u0026lt;- dat2 %\u0026gt;% rename(source=cat_location_d90, target=cat_location_d180_re , weight=n) # count the weight of each transition from day 180 to day 365; dat3\u0026lt;- data.frame(dplyr::count_(dat12, vars = c(\u0026#39;cat_location_d180_re\u0026#39;,\u0026#39;cat_location_d365_re\u0026#39;))) dat3 \u0026lt;- dat3 %\u0026gt;% rename(source=cat_location_d180_re, target=cat_location_d365_re , weight=n) # combine the data above into one links data; links2 \u0026lt;-data.frame (rbind(dat1, dat2, dat3)) # prepare the variables for sankeynetwork function; links2$source= as.numeric(links2$source) links2$target= as.numeric(links2$target) nodes$group \u0026lt;- as.factor(nodes$group ) # Give a color for each group: my_color \u0026lt;- \u0026#39;d3.scaleOrdinal() .domain([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;,\u0026quot;d\u0026quot;,\u0026quot;e\u0026quot;,\u0026quot;f\u0026quot;,\u0026quot;g\u0026quot;,\u0026quot;h\u0026quot;,\u0026quot;i\u0026quot;,\u0026quot;j\u0026quot;]) .range([\u0026quot;#69b3a2\u0026quot;, \u0026quot;steelblue\u0026quot;, \u0026quot;grey\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;pink\u0026quot;,\u0026quot;brown\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;red\u0026quot;,\u0026quot;yellow\u0026quot;])\u0026#39; # make the sankey diagram; p \u0026lt;- sankeyNetwork(Links = links2, Nodes = nodes, Source = \u0026quot;source\u0026quot;, Target = \u0026quot;target\u0026quot;, Value = \u0026quot;weight\u0026quot;, NodeID = \u0026quot;name\u0026quot;, fontSize = 11, nodeWidth =20, colourScale=my_color,NodeGroup=\u0026quot;group\u0026quot;, sinksRight=FALSE) p  "},{"id":3,"href":"/posts/plot/sankey/sankey-020722/","title":"Sankey diagram (R)","section":"Posts","content":" A Sankey Diagram is a popular data visualization for data flows display. For example, hospitals from better, average, worse performance categories can fall into different performance categories after quality improvement efforts. A Sankey diagram can be used to display such a change in categories. The categorical buckets (nodes) are represented by boxes or text at each column in the plot. The transition across nodes are connected by links. These links have a width proportional to the weight of the flow, which could be percentage the or number of subjects.\nThe R package (networkD3) can be used to create a sankey diagram. The key is to construct two dataset: 1) nodes: coding each bucket at different column in a sequential order, a group variable can be added; 2) links: the connections across different nodes. A detailed reference and examples for the R package can be reviewed here: https://www.data-to-viz.com/graph/sankey.html\nA data example: The goal of the following codes are to describe the change of locations after a left ventricular assist device (LVAD) implant (day 0) at day 90, 180 and 365. These possible locations such as home, SNF, hospitals, death, etc are coded as 1 to 9. Given the sensitivity of the data, the following codes may not be comprehensible without the data context.\n#the nodes data can be managed manually, the key is that the first entry would be recognized as 0 in the sankey diagram function, thus the coding order has to be started with 0; nodes\u0026lt;- read.csv (\u0026quot;.../nodes2.csv\u0026quot;) dat\u0026lt;- read.csv (\u0026quot;../final_location_4dates_w.csv\u0026quot;) library(tidyverse) dat$cat_location_d0_re=0 dat$cat_location_d90 [is.na(dat$cat_location_d90)]=9 # coding the NA subject as censor; dat$cat_location_d180 [is.na(dat$cat_location_d180)]=9 # coding the NA subject as censor; dat$cat_location_d365 [is.na(dat$cat_location_d365)]=9 # coding the NA subject as censor; # recode the locations at day 180 and day 365 to the sequential coding (day 90 is already set); dat11\u0026lt;- dat %\u0026gt;% mutate(cat_location_d180_re = recode(cat_location_d180, \u0026quot;1\u0026quot;=\u0026quot;10\u0026quot;,\u0026quot;2\u0026quot;=\u0026quot;11\u0026quot;, \u0026quot;3\u0026quot;=\u0026quot;12\u0026quot;,\u0026quot;4\u0026quot;=\u0026quot;13\u0026quot;,\u0026quot;5\u0026quot;=\u0026quot;14\u0026quot;, \u0026quot;6\u0026quot;=\u0026quot;15\u0026quot;, \u0026quot;7\u0026quot;=\u0026quot;16\u0026quot;,\u0026quot;8\u0026quot;=\u0026quot;17\u0026quot;, \u0026quot;9\u0026quot;=\u0026quot;18\u0026quot;)) dat12 \u0026lt;- dat11 %\u0026gt;% mutate(cat_location_d365_re = recode(cat_location_d365, \u0026quot;1\u0026quot;=\u0026quot;19\u0026quot;,\u0026quot;2\u0026quot;=\u0026quot;20\u0026quot;, \u0026quot;3\u0026quot;=\u0026quot;21\u0026quot;,\u0026quot;4\u0026quot;=\u0026quot;22\u0026quot;,\u0026quot;5\u0026quot;=\u0026quot;23\u0026quot;, \u0026quot;6\u0026quot;=\u0026quot;24\u0026quot;, \u0026quot;7\u0026quot;=\u0026quot;25\u0026quot;,\u0026quot;8\u0026quot;=\u0026quot;26\u0026quot;, \u0026quot;9\u0026quot;=\u0026quot;27\u0026quot;)) # count the weight of each transition from day 0 (d0) to day 90 (d90); dat1\u0026lt;- data.frame(dplyr::count_(dat12, vars = c(\u0026#39;cat_location_d0_re\u0026#39;,\u0026#39;cat_location_d90\u0026#39;))) # rename the variables; dat1 \u0026lt;- dat1 %\u0026gt;% rename(source=cat_location_d0_re, target=cat_location_d90 , weight=n) # count the weight of each transition from day 90 to day 180; dat2\u0026lt;- data.frame(dplyr::count_(dat12, vars = c(\u0026#39;cat_location_d90\u0026#39;,\u0026#39;cat_location_d180_re\u0026#39;))) dat2 \u0026lt;- dat2 %\u0026gt;% rename(source=cat_location_d90, target=cat_location_d180_re , weight=n) # count the weight of each transition from day 180 to day 365; dat3\u0026lt;- data.frame(dplyr::count_(dat12, vars = c(\u0026#39;cat_location_d180_re\u0026#39;,\u0026#39;cat_location_d365_re\u0026#39;))) dat3 \u0026lt;- dat3 %\u0026gt;% rename(source=cat_location_d180_re, target=cat_location_d365_re , weight=n) # combine the data above into one links data; links2 \u0026lt;-data.frame (rbind(dat1, dat2, dat3)) # prepare the variables for sankeynetwork function; links2$source= as.numeric(links2$source) links2$target= as.numeric(links2$target) nodes$group \u0026lt;- as.factor(nodes$group ) # Give a color for each group: my_color \u0026lt;- \u0026#39;d3.scaleOrdinal() .domain([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;,\u0026quot;d\u0026quot;,\u0026quot;e\u0026quot;,\u0026quot;f\u0026quot;,\u0026quot;g\u0026quot;,\u0026quot;h\u0026quot;,\u0026quot;i\u0026quot;,\u0026quot;j\u0026quot;]) .range([\u0026quot;#69b3a2\u0026quot;, \u0026quot;steelblue\u0026quot;, \u0026quot;grey\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;pink\u0026quot;,\u0026quot;brown\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;red\u0026quot;,\u0026quot;yellow\u0026quot;])\u0026#39; # make the sankey diagram; p \u0026lt;- sankeyNetwork(Links = links2, Nodes = nodes, Source = \u0026quot;source\u0026quot;, Target = \u0026quot;target\u0026quot;, Value = \u0026quot;weight\u0026quot;, NodeID = \u0026quot;name\u0026quot;, fontSize = 11, nodeWidth =20, colourScale=my_color,NodeGroup=\u0026quot;group\u0026quot;, sinksRight=FALSE) p "},{"id":4,"href":"/docs/mixed-effect-model-icc/mixed-effect-and-icc81121/","title":"Mixed effect model simulation and ICC (R)","section":"Docs","content":" Mixed effect model simulation and ICC Data example: Mixed effect model was simulated using parameters obtained from a pilot data. Intraclass correlation coefficient was calculated from the mixed effect model to test the agreement between raters. In this example, videos as the subjects were assessed by different raters, each rater will be assigned to multiple videos. nstand is parameter for the number of videos which will be assessed by various raters (nraters). Given the variation of the rating can be attributed to videos and raters. The mixed effect model in this simulation included videos and raters as two separate random intercepts. Intraclass correlation is calculated from the models with the simulated data. We varied the number of videos in the simulation. The ICC distribution plot from the simulated data showed that 60 videos has reached a more accurate estimate for ICC with the narrower range.\n video_fun = function( nstand = 30 ,# number of videos to rate nraters = (nstand/10)*3, #each rater rated 10 subjects (i.e.videos), and each subject is rated by 3 different raters. #nraters = (nstand/4)*3 #each rater rated 4 subjects, and each subject is rated by 3 different raters. nraterper=3 ,# the number of raters for each video; mu = 20.2, sds =9.9, # video random effect sd =3.2 , # residual rsd=3.6, # rater random effect nstandper=10 ) { #set.seed(201) rater_sample = sample(1:50, nraters, replace=F) rater = rep(rater_sample, each=10) # each rater repeats 10 times, since each rater reviews 10subjects; ratereff = rnorm(nraters, 0, rsd) # rsd the rater random effect; each rater has an random intercept ratereff = rep(ratereff,each=10) #set.seed(1917) x = sample(1:nstand,nstand) subject_assingment = c(x, c(x[nstand], x[-nstand]), c(x[(nstand - 1):nstand], x[1:(nstand - 2)])) standeff = rnorm(nstand, 0, sds) standeff =c(standeff, c(standeff[nstand], standeff[-nstand]), c(standeff[(nstand - 1):nstand], standeff[1:(nstand - 2)])) # sds the video random effect ; each video has an random intercept ploteff = rnorm(nstand*nraterper, 0, sd) # sd the overall residual effect dat2 = data.frame(rater,ratereff,subject_assingment,standeff,ploteff) dat2$resp = with(dat2, mu + standeff +ratereff+ploteff ) dat2$rater \u0026lt;-as.factor(dat2$rater) dat2$subject_assingment = as.factor(dat2$subject_assingment) fit1= lmer(resp ~ 1+(1|rater)+(1|subject_assingment), data = dat2, control=lmerControl(optimizer=\u0026quot;bobyqa\u0026quot;, optCtrl=list(maxfun=2e5))) vcv_= as.data.frame(t((as.data.frame (VarCorr (fit1))[c(\u0026#39;grp\u0026#39;,\u0026quot;vcov\u0026quot;)]) [,-1]) ) # exact the variation, transpose the group variable and variance columns. vcv \u0026lt;- as.data.frame (VarCorr (fit1)) names(vcv_) \u0026lt;- vcv[,1] # assign names for the variance in vcv_ data icc=vcv_$subject_assingment / (vcv_$subject_assingment+vcv_$rater +vcv_$Residual) # calculate the ICC, which the the variance from the subjects (videos) divided by the total variances. } library (dplyr) library (lme4) # call the function to calculate the ICC; sims = replicate(100, video_fun(), simplify = FALSE ) library(purrr) # v. 0.3.3 suppressPackageStartupMessages( library(dplyr) ) # v. 0.8.3 library(ggplot2) # v. 3.2.1 stand_sims = c(30,60) %\u0026gt;% set_names() %\u0026gt;% map(~replicate(100, video_fun(nstand = .x) ) ) # repeat the function for 100 times with different nstand (# of videos) # convert the list to dataframe df3=as.data.frame(do.call(cbind, stand_sims)) summary (df3$`30`) # summarize the ICC ranges by samples size library (reshape2) # reshape the dataframe df3 \u0026lt;- melt(df3) # plot the distribution by various number of videos # ICC=0.81 is the ICC from pilot data and used as the reference here. ggplot(df3, aes(x = value) ) +xlim(0.1,1.0)+ geom_density(fill = \u0026quot;blue\u0026quot;, alpha = .25) + facet_wrap(~variable) + geom_vline(xintercept = 0.81)  "},{"id":5,"href":"/posts/mixed-effect-model-icc/mixed-effect-and-icc81121/","title":"Mixed effect model simulation and ICC (R)","section":"Posts","content":" Data example: Mixed effect model was simulated using parameters obtained from a pilot data. Intraclass correlation coefficient was calculated from the mixed effect model to test the agreement between raters. In this example, videos as the subjects were assessed by different raters, each rater will be assigned to multiple videos. nstand is parameter for the number of videos which will be assessed by various raters (nraters). Given the variation of the rating can be attributed to videos and raters. The mixed effect model in this simulation included videos and raters as two separate random intercepts. Intraclass correlation is calculated from the models with the simulated data. We varied the number of videos in the simulation. The ICC distribution plot from the simulated data showed that 60 videos has reached a more accurate estimate for ICC with the narrower range.\n video_fun = function( nstand = 30 ,# number of videos to rate nraters = (nstand/10)*3, #each rater rated 10 subjects (i.e.videos), and each subject is rated by 3 different raters. #nraters = (nstand/4)*3 #each rater rated 4 subjects, and each subject is rated by 3 different raters. nraterper=3 ,# the number of raters for each video; mu = 20.2, sds =9.9, # video random effect sd =3.2 , # residual rsd=3.6, # rater random effect nstandper=10 ) { #set.seed(201) rater_sample = sample(1:50, nraters, replace=F) rater = rep(rater_sample, each=10) # each rater repeats 10 times, since each rater reviews 10subjects; ratereff = rnorm(nraters, 0, rsd) # rsd the rater random effect; each rater has an random intercept ratereff = rep(ratereff,each=10) #set.seed(1917) x = sample(1:nstand,nstand) subject_assingment = c(x, c(x[nstand], x[-nstand]), c(x[(nstand - 1):nstand], x[1:(nstand - 2)])) standeff = rnorm(nstand, 0, sds) standeff =c(standeff, c(standeff[nstand], standeff[-nstand]), c(standeff[(nstand - 1):nstand], standeff[1:(nstand - 2)])) # sds the video random effect ; each video has an random intercept ploteff = rnorm(nstand*nraterper, 0, sd) # sd the overall residual effect dat2 = data.frame(rater,ratereff,subject_assingment,standeff,ploteff) dat2$resp = with(dat2, mu + standeff +ratereff+ploteff ) dat2$rater \u0026lt;-as.factor(dat2$rater) dat2$subject_assingment = as.factor(dat2$subject_assingment) fit1= lmer(resp ~ 1+(1|rater)+(1|subject_assingment), data = dat2, control=lmerControl(optimizer=\u0026quot;bobyqa\u0026quot;, optCtrl=list(maxfun=2e5))) vcv_= as.data.frame(t((as.data.frame (VarCorr (fit1))[c(\u0026#39;grp\u0026#39;,\u0026quot;vcov\u0026quot;)]) [,-1]) ) # exact the variation, transpose the group variable and variance columns. vcv \u0026lt;- as.data.frame (VarCorr (fit1)) names(vcv_) \u0026lt;- vcv[,1] # assign names for the variance in vcv_ data icc=vcv_$subject_assingment / (vcv_$subject_assingment+vcv_$rater +vcv_$Residual) # calculate the ICC, which the the variance from the subjects (videos) divided by the total variances. } library (dplyr) library (lme4) # call the function to calculate the ICC; sims = replicate(100, video_fun(), simplify = FALSE ) library(purrr) # v. 0.3.3 suppressPackageStartupMessages( library(dplyr) ) # v. 0.8.3 library(ggplot2) # v. 3.2.1 stand_sims = c(30,60) %\u0026gt;% set_names() %\u0026gt;% map(~replicate(100, video_fun(nstand = .x) ) ) # repeat the function for 100 times with different nstand (# of videos) # convert the list to dataframe df3=as.data.frame(do.call(cbind, stand_sims)) summary (df3$`30`) # summarize the ICC ranges by samples size library (reshape2) # reshape the dataframe df3 \u0026lt;- melt(df3) # plot the distribution by various number of videos # ICC=0.81 is the ICC from pilot data and used as the reference here. ggplot(df3, aes(x = value) ) +xlim(0.1,1.0)+ geom_density(fill = \u0026quot;blue\u0026quot;, alpha = .25) + facet_wrap(~variable) + geom_vline(xintercept = 0.81) "},{"id":6,"href":"/docs/psm/psm/psm/","title":"1. Propensity score methods-I PS Matching (SAS)","section":"Propensity score methods","content":" Propensity score methods include 1) propensity score matching; 2) stratification on the propensity score; 3)IPTW: inverse probability of treatment weighting; 4) covariate adjustment using the propensity score. The codes below only apply to propensity score matching.\nVariables consideration for propensity score model: To correctly specify the propensity score model, one should include the true confounders (the risk factors that are associated with the ‘treatment’ and outcome) and the potential confounders (the risk facotrs that are associated with the outcome). As no one knows the true model, a safe way to do (when sample size is not a concern) is to include all baseline risk factors. This way could potentially reduce the matching rate expecially when sample size is small.\n To check balance of propensity score matched sample, one should use standardized difference. A threshold of less than 10% of standardized difference may be used to consider balancing between treatment and control group. SAS defaultly used 0.25 as the cutoff for standardized difference. Using p-values to determine balance is discouraged given that 1)p-values are sensitive to sample size. 2) p values are inference from a “superpopulation”, the balance is an attribute to the particular matched sample. One could calculate the standardized difference with the following codes, or use the following formula.\n  The standardized difference for a continous variable is\n\\[ d= \\frac {\\overline{x}_{treatment}-\\overline{x}_{control}}{\\sqrt {\\frac{s^2_{treatment}+s^2_{control}}{2}}} \\]\nThe standardized difference for a binary variable is\n\\[ d=\\frac {\\hat{p}_{treatment}-\\hat{p}_{control}}{\\sqrt {\\frac{\\hat{p}_{treatment} (1-\\hat{p}_{treatment})+ \\hat{p}_{control} (1-\\hat{p}_{control})}{2}}} \\]\nReference: Peter C. Austin.An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies.\nOutcome analyis after matching  3.1 After you obtain a matched sample and verify the balancing of the two treatment groups of interest, you may proceed to outcome analysis using the matched data. Some suggest that conditional logistic model or paired t-tests may be needed to account for the matching pairs, while others suggest conditional on pairs is not necessary for the outcome analysis given the matched sample mimics randomized trial data. In a daily analysis, usual regression model or tests can be performed without accounting for the matching pairs.\n3.2 one to various ratio matching\nWhen one control is matched to many treatments, and the ratio is not constant, for example, 1:2, 1:1, 1:4 were all possible matching ratio. The outcome analysis tests need to be weighted by the number of controls. For example, if one treatment is matched to 4 controls, each control should be weighted as 1/4. The analysis codes are often needed an observation weight statement as shown below in 4.1 code example.\nImplementation  4.1 A Sas procedure: SAS psmatch procedure provides convienient implementation for these PS methods, the output also quantifies the standardized difference. Here is an code example for one to many matching.\n/* propensity score model for group as the treatment, matching variables: age, gender, COPD and acute stroke */ proc psmatch data=aortic; class group gender_ COPD AcuteStroke ; psmodel group (treated=\u0026#39;1\u0026#39;)= age_at_operation gender_ COPD AcuteStroke; match method=varratio(kmin=1 kmax=3) caliper=0.5; assess lps var=(age_at_operation gender_ COPD AcuteStroke ) / plots=StdDiff; output out(obs=match)=Matched1N1k ATTWGT=attwgt matchid=mid; run; * note that _MATCHWGT_ indicates the weighting based on the matching ratio; * attwgt indicates the weights based on propensity score to calculate average treatment effect on the treated (ATT), atewgt indicates the weights to calculate average treatment effect (ATE). * for IPTW to assess the balance of the weighted sample, in the assess statement , WEIGHT= ATTWGT or ATEWGT could be used to diagnosis the balance. *outcome analysis; proc freq data=Matched1N1k; table group*Mt30Stat / nocol nopercent cmh; weight _MATCHWGT_; run; proc sort data=Matched1N1k; by mid;run; proc ttest data=Matched1N1k; class group; var age_at_operation; weight _MATCHWGT_; run; 4.2. STATA: STATA provide psmatch2, teffects command. The output has ATT, ATE calculated. The command is simple and straightforward, but it is difficult to customize to meet additional analytic need, such as extracting the matched data.\n4.3 A SAS macro: The below uses a SAS macro to find matching samples. The pros of using this extensive macro code is that it allows you to modify the matching criterials. For example, if one wants to do matching within a same center, you may modify the macro matching condition.\n/* modify the macro matching condition as the following: */ %if %upcase(\u0026amp;method) = CALIPER %then %do; if (pscoreT - \u0026amp;caliper) \u0026lt;= pscoreC \u0026lt;= (pscoreT + \u0026amp;caliper) and center_idC=center_idT then do; Here is the code example of using a sas macro for PSM method;\n/*using the logistic model to build propensity score model for having autologous blood (variable: autoblood) /*(autologous blood is done before cardiac surgery to collect the patients own blood); * rename autoblood as the treatment group; data dat; set dat; group=AutoBlood; run; * assign the subject id; data dat; set dat; subject_id=_n_;run; * the propensity score model for treatment group; proc logistic data=dat descending outest=betas; class gender CAD_ Any_RF_Preop AcuteStroke PriorCardSx cardiogenicShock severe_AI /param=ref ; model AutoBlood (event=\u0026#39;1\u0026#39;)= age_at_operation BMI RFHemoglobin_ gender AcuteStroke PriorCardSx cardiogenicShock ; *output the dataset \u0026quot;predic\u0026quot; which have predprob: predprob is the probability scale, and xbeta is the linear predictor; output out=predic p=predprob xbeta=logit; run; * if using the probability scale as the propensity score to do matching, then using predprob as the pscoreT/C; * if using the logit scale as the propensity score for matching, then using xbeta as the pscoreT/C; data treatment(rename =(subject_id=idT logit= pscoreT ) );set predic; where group=1 ; run; data control (rename =(subject_id=idC logit= pscoreC) ); set predic; where group=0; run; *********** * when matching within center, you will need to assign center id to each center, and modify the macro codes; data treatment(rename =(subject_id=idT predprob_r= pscoreT center_id=center_idT ) );set predic2; where treat=1 ; run; data control (rename =(subject_id=idC predprob_r= pscoreC center_id=center_idC)); set predic2; where treat=0; run; ***************** * using the propensity score matching macro; %include \u0026#39;xx\\propensity score matching macro\\psmatching.sas\u0026#39;; proc means data=predic; var logit; output out=stddata (keep =std) std=std;run; * using the 0.1 standardard deviation of the propensity score as the caliper for finding matching; data stddata; set stddata; caliper=0.1*std;run; *one-to-one matching, no replacement, caliper matching; %PSMatching(datatreatment= treatment, datacontrol= control, method= CALIPER, numberofcontrols=1, caliper=0.0844, replacement= no, out= caliper_matches2); data treatment2 (keep=idT rename=(idT=MatchedToTreatID) ); set treatment;run; data control2 (keep=idC rename=(idC=IdSelectedControl )); set control;run; proc sort data=caliper_matches2 nodupkey out=test2; by MatchedToTreatID;run; * create the data for matching sample; data caliper_pairs; set caliper_matches2; subject_id = IdSelectedControl; pscore = PScoreControl; pair = _N_; output; subject_id = MatchedToTreatID; pscore = PScoreTreat; pair = _N_; output; keep subject_id pscore pair;run; * merging the dataset with the original sample to have the other covariates; proc sort data=dat; by subject_id;run; proc sort data=caliper_pairs; by subject_id;run; data merge_pairs; merge caliper_pairs (in=a) dat (in=b); by subject_id; if a=1 and b=1;run; *check balance for continuous variables by plotting the empirical distribution between treatment and control groups ; proc npar1way d edf plots=edfplot data=merge_pairs scores=data; class group; var pscore age_at_operation BMI RFHemoglobin_ ; run; proc sort data=merge_pairs; by group;run; proc means data=merge_pairs mean median p25 p75; var age_ pre_op_creatinine ef__pre_; class group; run; * MACRO: use standardized difference for checking balance for binary variables; %macro binary (var=, label=); proc means data=merge_pairs mean noprint; var \u0026amp;var; by group; output out=outmean (keep=group mean ) mean=mean ; run; data del0; set outmean; if group=0; mean_0 =mean; keep mean_0;run; data del1; set outmean; if group=1; mean_1 =mean; keep mean_1;run; data newdata; length label $25; merge del0 del1; d= (mean_1-mean_0)/sqrt ((mean_1*(1-mean_1)+mean_0*(1-mean_0))/2); d=round(abs(d),0.0001); label=\u0026amp;label; keep d label; run; proc append data=newdata base=standiff (keep=d label) force;run; %mend binary; proc sort data=merge_pairs; by group;run; * make sure the variables were coded as numeric variables; data merge_pairs; set merge_pairs; if gender=\u0026quot;1\u0026quot; then gender_=1; else gender_=0;run; %binary (var=gender_, label=\u0026quot;gender\u0026quot;) %binary (var=CAD_, label=\u0026quot;CAD_\u0026quot;) %binary (var=severe_AI, label=\u0026quot;severe_AI\u0026quot;) proc print data=standiff;run; * using the matching samples, look at outcome difference with the conditional logistic model; proc logistic data=merge_pairs; class group (ref=\u0026#39;0\u0026#39;); model OperativeMortality (event=\u0026#39;1\u0026#39;)=group; strata pair; run; * using the matching samples, look at survival difference with cox model accounting for the pairs; proc phreg data=merge_pairs ; class group (ref=\u0026#39;0\u0026#39;) pair; model time_since_surgery*death (0)=group; random pair; hazardratio group/diff=ref; run; The below is the propensity score matching MACRO as referred above:\n#SAS Macro for obtaining propensity score matching samples; /************************************************ PSmatching.sas adapted from Paper 185-2007 SAS Global Forum 2007 Local and Global Optimal Propensity Score Matching Marcelo Coca-Perraillon Health Care Policy Department, Harvard Medical School, Boston, MA ------------------------------- Treatment and Control observations must be in separate datasets such that Control data includes: idC = subject_id, pscoreC = propensity score Treatment data includes: idT, pscoreT id must be numeric method = NN (nearest neighbor), caliper, or radius caliper value = max for matching replacement = yes/no whether controls can be matched to more than one case out = output data set name example call: %PSMatching(datatreatment= T, datacontrol= C, method= NN, numberofcontrols= 1, caliper=, replacement= no, out= matches); Output format: Id Matched Selected PScore To PScore Obs Control Control TreatID Treat 1 18628 0.39192 16143 0.39192 2 18505 0.23029 16158 0.23002 3 15589 0.29260 16112 0.29260 All other variables discarded. Reformat for merge on subject_id with original data: data pairs; set matches; subject_id = IdSelectedControl; pscore = PScoreControl; pair = _N_; output; subject_id = MatchedToTreatID; pscore = PScoreTreat; pair = _N_; output; keep subject_id pscore pair; ************************************************/ %macro PSMatching(datatreatment=, datacontrol=, method=, numberofcontrols=, caliper=, replacement=, out=); /* Create copies of the treated units if N \u0026gt; 1 */; data _Treatment0(drop= i); set \u0026amp;datatreatment; do i= 1 to \u0026amp;numberofcontrols; RandomNumber= ranuni(12345); output; end; run; /* Randomly sort both datasets */ proc sort data= _Treatment0 out= _Treatment(drop= RandomNumber); by RandomNumber; run; data _Control0; set \u0026amp;datacontrol; RandomNumber= ranuni(45678); run; proc sort data= _Control0 out= _Control(drop= RandomNumber); by RandomNumber; run; data Matched(keep = IdSelectedControl PScoreControl MatchedToTreatID PScoreTreat); length pscoreC 8; length idC 8; /* Load Control dataset into the hash object */ if _N_= 1 then do; declare hash h(dataset: \u0026quot;_Control\u0026quot;, ordered: \u0026#39;no\u0026#39;); declare hiter iter(\u0026#39;h\u0026#39;); h.defineKey(\u0026#39;idC\u0026#39;); h.defineData(\u0026#39;pscoreC\u0026#39;, \u0026#39;idC\u0026#39;); h.defineDone(); call missing(idC, pscoreC); end; /* Open the treatment */ set _Treatment; %if %upcase(\u0026amp;method) ~= RADIUS %then %do; retain BestDistance 99; %end; /* Iterate over the hash */ rc= iter.first(); if (rc=0) then BestDistance= 99; do while (rc = 0); /* Caliper */ %if %upcase(\u0026amp;method) = CALIPER %then %do; if (pscoreT - \u0026amp;caliper) \u0026lt;= pscoreC \u0026lt;= (pscoreT + \u0026amp;caliper) then do; ScoreDistance = abs(pscoreT - pscoreC); if ScoreDistance \u0026lt; BestDistance then do; BestDistance = ScoreDistance; IdSelectedControl = idC; PScoreControl = pscoreC; MatchedToTreatID = idT; PScoreTreat = pscoreT; end; end; %end; /* NN */ %if %upcase(\u0026amp;method) = NN %then %do; ScoreDistance = abs(pscoreT - pscoreC); if ScoreDistance \u0026lt; BestDistance then do; BestDistance = ScoreDistance; IdSelectedControl = idC; PScoreControl = pscoreC; MatchedToTreatID = idT; PScoreTreat = pscoreT; end; %end; %if %upcase(\u0026amp;method) = NN or %upcase(\u0026amp;method) = CALIPER %then %do; rc = iter.next(); /* Output the best control and remove it */ if (rc ~= 0) and BestDistance ~=99 then do; output; %if %upcase(\u0026amp;replacement) = NO %then %do; rc1 = h.remove(key: IdSelectedControl); %end; end; %end; /* Radius */ %if %upcase(\u0026amp;method) = RADIUS %then %do; if (pscoreT - \u0026amp;caliper) \u0026lt;= pscoreC \u0026lt;= (pscoreT + \u0026amp;caliper) then do; IdSelectedControl = idC; PScoreControl = pscoreC; MatchedToTreatID = idT; PScoreTreat = pscoreT; output; end; rc = iter.next(); %end; end; run; /* Delete temporary tables. Quote for debugging */ proc datasets; delete _:(gennum=all); run; data \u0026amp;out; set Matched; run; %mend PSMatching; "},{"id":7,"href":"/posts/psm/psm/","title":"Propensity score methods-I PS Matching (SAS)","section":"Posts","content":" Propensity score methods include 1) propensity score matching; 2) stratification on the propensity score; 3)IPTW: inverse probability of treatment weighting; 4) covariate adjustment using the propensity score. The codes below only apply to propensity score matching.\nVariables consideration for propensity score model: To correctly specify the propensity score model, one should include the true confounders (the risk factors that are associated with the ‘treatment’ and outcome) and the potential confounders (the risk facotrs that are associated with the outcome). As no one knows the true model, a safe way to do (when sample size is not a concern) is to include all baseline risk factors. This way could potentially reduce the matching rate expecially when sample size is small.\n To check balance of propensity score matched sample, one should use standardized difference. A threshold of less than 10% of standardized difference may be used to consider balancing between treatment and control group. SAS defaultly used 0.25 as the cutoff for standardized difference. Using p-values to determine balance is discouraged given that 1)p-values are sensitive to sample size. 2) p values are inference from a “superpopulation”, the balance is an attribute to the particular matched sample. One could calculate the standardized difference with the following codes, or use the following formula.\n  The standardized difference for a continous variable is\n\\[ d= \\frac {\\overline{x}_{treatment}-\\overline{x}_{control}}{\\sqrt {\\frac{s^2_{treatment}+s^2_{control}}{2}}} \\]\nThe standardized difference for a binary variable is\n\\[ d=\\frac {\\hat{p}_{treatment}-\\hat{p}_{control}}{\\sqrt {\\frac{\\hat{p}_{treatment} (1-\\hat{p}_{treatment})+ \\hat{p}_{control} (1-\\hat{p}_{control})}{2}}} \\]\nReference: Peter C. Austin.An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies.\nOutcome analyis after matching  3.1 After you obtain a matched sample and verify the balancing of the two treatment groups of interest, you may proceed to outcome analysis using the matched data. Some suggest that conditional logistic model or paired t-tests may be needed to account for the matching pairs, while others suggest conditional on pairs is not necessary for the outcome analysis given the matched sample mimics randomized trial data. In a daily analysis, usual regression model or tests can be performed without accounting for the matching pairs.\n3.2 one to various ratio matching\nWhen one control is matched to many treatments, and the ratio is not constant, for example, 1:2, 1:1, 1:4 were all possible matching ratio. The outcome analysis tests need to be weighted by the number of controls. For example, if one treatment is matched to 4 controls, each control should be weighted as 1/4. The analysis codes are often needed an observation weight statement as shown below in 4.1 code example.\nImplementation  4.1 A Sas procedure: SAS psmatch procedure provides convienient implementation for these PS methods, the output also quantifies the standardized difference. Here is an code example for one to many matching.\n/* propensity score model for group as the treatment, matching variables: age, gender, COPD and acute stroke */ proc psmatch data=aortic; class group gender_ COPD AcuteStroke ; psmodel group (treated=\u0026#39;1\u0026#39;)= age_at_operation gender_ COPD AcuteStroke; match method=varratio(kmin=1 kmax=3) caliper=0.5; assess lps var=(age_at_operation gender_ COPD AcuteStroke ) / plots=StdDiff; output out(obs=match)=Matched1N1k ATTWGT=attwgt matchid=mid; run; * note that _MATCHWGT_ indicates the weighting based on the matching ratio; * attwgt indicates the weights based on propensity score to calculate average treatment effect on the treated (ATT), atewgt indicates the weights to calculate average treatment effect (ATE). * for IPTW to assess the balance of the weighted sample, in the assess statement , WEIGHT= ATTWGT or ATEWGT could be used to diagnosis the balance. *outcome analysis; proc freq data=Matched1N1k; table group*Mt30Stat / nocol nopercent cmh; weight _MATCHWGT_; run; proc sort data=Matched1N1k; by mid;run; proc ttest data=Matched1N1k; class group; var age_at_operation; weight _MATCHWGT_; run; 4.2. STATA: STATA provide psmatch2, teffects command. The output has ATT, ATE calculated. The command is simple and straightforward, but it is difficult to customize to meet additional analytic need, such as extracting the matched data.\n4.3 A SAS macro: The below uses a SAS macro to find matching samples. The pros of using this extensive macro code is that it allows you to modify the matching criterials. For example, if one wants to do matching within a same center, you may modify the macro matching condition.\n/* modify the macro matching condition as the following: */ %if %upcase(\u0026amp;method) = CALIPER %then %do; if (pscoreT - \u0026amp;caliper) \u0026lt;= pscoreC \u0026lt;= (pscoreT + \u0026amp;caliper) and center_idC=center_idT then do; Here is the code example of using a sas macro for PSM method;\n/*using the logistic model to build propensity score model for having autologous blood (variable: autoblood) /*(autologous blood is done before cardiac surgery to collect the patients own blood); * rename autoblood as the treatment group; data dat; set dat; group=AutoBlood; run; * assign the subject id; data dat; set dat; subject_id=_n_;run; * the propensity score model for treatment group; proc logistic data=dat descending outest=betas; class gender CAD_ Any_RF_Preop AcuteStroke PriorCardSx cardiogenicShock severe_AI /param=ref ; model AutoBlood (event=\u0026#39;1\u0026#39;)= age_at_operation BMI RFHemoglobin_ gender AcuteStroke PriorCardSx cardiogenicShock ; *output the dataset \u0026quot;predic\u0026quot; which have predprob: predprob is the probability scale, and xbeta is the linear predictor; output out=predic p=predprob xbeta=logit; run; * if using the probability scale as the propensity score to do matching, then using predprob as the pscoreT/C; * if using the logit scale as the propensity score for matching, then using xbeta as the pscoreT/C; data treatment(rename =(subject_id=idT logit= pscoreT ) );set predic; where group=1 ; run; data control (rename =(subject_id=idC logit= pscoreC) ); set predic; where group=0; run; *********** * when matching within center, you will need to assign center id to each center, and modify the macro codes; data treatment(rename =(subject_id=idT predprob_r= pscoreT center_id=center_idT ) );set predic2; where treat=1 ; run; data control (rename =(subject_id=idC predprob_r= pscoreC center_id=center_idC)); set predic2; where treat=0; run; ***************** * using the propensity score matching macro; %include \u0026#39;xx\\propensity score matching macro\\psmatching.sas\u0026#39;; proc means data=predic; var logit; output out=stddata (keep =std) std=std;run; * using the 0.1 standardard deviation of the propensity score as the caliper for finding matching; data stddata; set stddata; caliper=0.1*std;run; *one-to-one matching, no replacement, caliper matching; %PSMatching(datatreatment= treatment, datacontrol= control, method= CALIPER, numberofcontrols=1, caliper=0.0844, replacement= no, out= caliper_matches2); data treatment2 (keep=idT rename=(idT=MatchedToTreatID) ); set treatment;run; data control2 (keep=idC rename=(idC=IdSelectedControl )); set control;run; proc sort data=caliper_matches2 nodupkey out=test2; by MatchedToTreatID;run; * create the data for matching sample; data caliper_pairs; set caliper_matches2; subject_id = IdSelectedControl; pscore = PScoreControl; pair = _N_; output; subject_id = MatchedToTreatID; pscore = PScoreTreat; pair = _N_; output; keep subject_id pscore pair;run; * merging the dataset with the original sample to have the other covariates; proc sort data=dat; by subject_id;run; proc sort data=caliper_pairs; by subject_id;run; data merge_pairs; merge caliper_pairs (in=a) dat (in=b); by subject_id; if a=1 and b=1;run; *check balance for continuous variables by plotting the empirical distribution between treatment and control groups ; proc npar1way d edf plots=edfplot data=merge_pairs scores=data; class group; var pscore age_at_operation BMI RFHemoglobin_ ; run; proc sort data=merge_pairs; by group;run; proc means data=merge_pairs mean median p25 p75; var age_ pre_op_creatinine ef__pre_; class group; run; * MACRO: use standardized difference for checking balance for binary variables; %macro binary (var=, label=); proc means data=merge_pairs mean noprint; var \u0026amp;var; by group; output out=outmean (keep=group mean ) mean=mean ; run; data del0; set outmean; if group=0; mean_0 =mean; keep mean_0;run; data del1; set outmean; if group=1; mean_1 =mean; keep mean_1;run; data newdata; length label $25; merge del0 del1; d= (mean_1-mean_0)/sqrt ((mean_1*(1-mean_1)+mean_0*(1-mean_0))/2); d=round(abs(d),0.0001); label=\u0026amp;label; keep d label; run; proc append data=newdata base=standiff (keep=d label) force;run; %mend binary; proc sort data=merge_pairs; by group;run; * make sure the variables were coded as numeric variables; data merge_pairs; set merge_pairs; if gender=\u0026quot;1\u0026quot; then gender_=1; else gender_=0;run; %binary (var=gender_, label=\u0026quot;gender\u0026quot;) %binary (var=CAD_, label=\u0026quot;CAD_\u0026quot;) %binary (var=severe_AI, label=\u0026quot;severe_AI\u0026quot;) proc print data=standiff;run; * using the matching samples, look at outcome difference with the conditional logistic model; proc logistic data=merge_pairs; class group (ref=\u0026#39;0\u0026#39;); model OperativeMortality (event=\u0026#39;1\u0026#39;)=group; strata pair; run; * using the matching samples, look at survival difference with cox model accounting for the pairs; proc phreg data=merge_pairs ; class group (ref=\u0026#39;0\u0026#39;) pair; model time_since_surgery*death (0)=group; random pair; hazardratio group/diff=ref; run; The below is the propensity score matching MACRO as referred above:\n#SAS Macro for obtaining propensity score matching samples; /************************************************ PSmatching.sas adapted from Paper 185-2007 SAS Global Forum 2007 Local and Global Optimal Propensity Score Matching Marcelo Coca-Perraillon Health Care Policy Department, Harvard Medical School, Boston, MA ------------------------------- Treatment and Control observations must be in separate datasets such that Control data includes: idC = subject_id, pscoreC = propensity score Treatment data includes: idT, pscoreT id must be numeric method = NN (nearest neighbor), caliper, or radius caliper value = max for matching replacement = yes/no whether controls can be matched to more than one case out = output data set name example call: %PSMatching(datatreatment= T, datacontrol= C, method= NN, numberofcontrols= 1, caliper=, replacement= no, out= matches); Output format: Id Matched Selected PScore To PScore Obs Control Control TreatID Treat 1 18628 0.39192 16143 0.39192 2 18505 0.23029 16158 0.23002 3 15589 0.29260 16112 0.29260 All other variables discarded. Reformat for merge on subject_id with original data: data pairs; set matches; subject_id = IdSelectedControl; pscore = PScoreControl; pair = _N_; output; subject_id = MatchedToTreatID; pscore = PScoreTreat; pair = _N_; output; keep subject_id pscore pair; ************************************************/ %macro PSMatching(datatreatment=, datacontrol=, method=, numberofcontrols=, caliper=, replacement=, out=); /* Create copies of the treated units if N \u0026gt; 1 */; data _Treatment0(drop= i); set \u0026amp;datatreatment; do i= 1 to \u0026amp;numberofcontrols; RandomNumber= ranuni(12345); output; end; run; /* Randomly sort both datasets */ proc sort data= _Treatment0 out= _Treatment(drop= RandomNumber); by RandomNumber; run; data _Control0; set \u0026amp;datacontrol; RandomNumber= ranuni(45678); run; proc sort data= _Control0 out= _Control(drop= RandomNumber); by RandomNumber; run; data Matched(keep = IdSelectedControl PScoreControl MatchedToTreatID PScoreTreat); length pscoreC 8; length idC 8; /* Load Control dataset into the hash object */ if _N_= 1 then do; declare hash h(dataset: \u0026quot;_Control\u0026quot;, ordered: \u0026#39;no\u0026#39;); declare hiter iter(\u0026#39;h\u0026#39;); h.defineKey(\u0026#39;idC\u0026#39;); h.defineData(\u0026#39;pscoreC\u0026#39;, \u0026#39;idC\u0026#39;); h.defineDone(); call missing(idC, pscoreC); end; /* Open the treatment */ set _Treatment; %if %upcase(\u0026amp;method) ~= RADIUS %then %do; retain BestDistance 99; %end; /* Iterate over the hash */ rc= iter.first(); if (rc=0) then BestDistance= 99; do while (rc = 0); /* Caliper */ %if %upcase(\u0026amp;method) = CALIPER %then %do; if (pscoreT - \u0026amp;caliper) \u0026lt;= pscoreC \u0026lt;= (pscoreT + \u0026amp;caliper) then do; ScoreDistance = abs(pscoreT - pscoreC); if ScoreDistance \u0026lt; BestDistance then do; BestDistance = ScoreDistance; IdSelectedControl = idC; PScoreControl = pscoreC; MatchedToTreatID = idT; PScoreTreat = pscoreT; end; end; %end; /* NN */ %if %upcase(\u0026amp;method) = NN %then %do; ScoreDistance = abs(pscoreT - pscoreC); if ScoreDistance \u0026lt; BestDistance then do; BestDistance = ScoreDistance; IdSelectedControl = idC; PScoreControl = pscoreC; MatchedToTreatID = idT; PScoreTreat = pscoreT; end; %end; %if %upcase(\u0026amp;method) = NN or %upcase(\u0026amp;method) = CALIPER %then %do; rc = iter.next(); /* Output the best control and remove it */ if (rc ~= 0) and BestDistance ~=99 then do; output; %if %upcase(\u0026amp;replacement) = NO %then %do; rc1 = h.remove(key: IdSelectedControl); %end; end; %end; /* Radius */ %if %upcase(\u0026amp;method) = RADIUS %then %do; if (pscoreT - \u0026amp;caliper) \u0026lt;= pscoreC \u0026lt;= (pscoreT + \u0026amp;caliper) then do; IdSelectedControl = idC; PScoreControl = pscoreC; MatchedToTreatID = idT; PScoreTreat = pscoreT; output; end; rc = iter.next(); %end; end; run; /* Delete temporary tables. Quote for debugging */ proc datasets; delete _:(gennum=all); run; data \u0026amp;out; set Matched; run; %mend PSMatching; "},{"id":8,"href":"/docs/collinearity/collinearity/","title":"Collinearity (SAS)","section":"Docs","content":" Collinearity Collinearity occurs when two variables are highly correlated with each other. Collinearity may cause inflation in standard errors for the given risk factors. However, this is not an issue for prediction purpose. If the collinearity doesn’t involve the risk factors of interest, it’s not a big issue to include the collinear variables for risk adjustment.\nTo check for collinearity, SAS PROC Reg has a convenient option. But SAS logistic or COX do not have corresponding option to check collinearity issues. As the collinearity validation does not depend on the model, it is ok to have the set of variables in PROC Reg (with a “makeup” outcome) to check for collinearity.\nproc reg data=dat ; model BMI =ef gender age_at_operation HTN PreopCRF_ HistoryofCVA PriorCardSx severeAI AcuteMI creatinine DebakeyDissectionClass cardiogenicshock AcuteParalysis/ tol vif collinoint; quit; For the collinearity test, the procedure will take one of variable as the outcome and regress it against all the other remaining variables.\nCollinearity is present when the VIF is higher than 5 to 10 or the condition indices are higher than 10 to 30. To look for the collinear variables, find the variables with the condition index higher than 10 to 30 and the proportion of variation are higher than 0.8 to 0.9 in the collinearity diagnostics table.\n "},{"id":9,"href":"/posts/collinearity/collinearity/","title":"Collinearity (SAS)","section":"Posts","content":" Collinearity occurs when two variables are highly correlated with each other. Collinearity may cause inflation in standard errors for the given risk factors. However, this is not an issue for prediction purpose. If the collinearity doesn’t involve the risk factors of interest, it’s not a big issue to include the collinear variables for risk adjustment.\nTo check for collinearity, SAS PROC Reg has a convenient option. But SAS logistic or COX do not have corresponding option to check collinearity issues. As the collinearity validation does not depend on the model, it is ok to have the set of variables in PROC Reg (with a “makeup” outcome) to check for collinearity.\nproc reg data=dat ; model BMI =ef gender age_at_operation HTN PreopCRF_ HistoryofCVA PriorCardSx severeAI AcuteMI creatinine DebakeyDissectionClass cardiogenicshock AcuteParalysis/ tol vif collinoint; quit; For the collinearity test, the procedure will take one of variable as the outcome and regress it against all the other remaining variables.\nCollinearity is present when the VIF is higher than 5 to 10 or the condition indices are higher than 10 to 30. To look for the collinear variables, find the variables with the condition index higher than 10 to 30 and the proportion of variation are higher than 0.8 to 0.9 in the collinearity diagnostics table.\n"},{"id":10,"href":"/docs/longitudinal-plot/longitudinal-plot/","title":"longitudinal plot (SAS)","section":"Docs","content":" Longitudinal plot About the data example: The aortic root sizes were measured over time for two groups of patients. The change of root size over time is of interest. In this analysis, we first plotted the group mean at each discrete time to visualize the root growth, secondly modeled the continuous time effect of root growth. The data is setup in a long format that each subject with ID (sBAV_ID_Number) has multiple rows with multiple measurements at different times.\nTo generate longitudinal plots:  *plot the group mean over time with formatting; goptions reset=all ftext=swiss ; axis1 label=(h=2 a=90 \u0026quot;Root size (mm)\u0026quot;) value=( h=2 ); axis2 label= (h=2 \u0026quot;Time (year)\u0026quot;) value=( h=2 ); legend1 label=(f=swiss h=2 \u0026#39;Root Group\u0026#39;) value=(h=2 color=black \u0026#39;root control\u0026#39; \u0026#39;root dilation\u0026#39; ) ; * time2 is a decrete time variable in the dataset; proc sort data=rootdat out=rootdat_t nodupkey; by sBAV_ID_Number time2;run; proc gplot data=rootdat_t; plot root_size*time2=Root_group / haxis=axis2 vaxis=axis1 legend = legend1 ; symbol1 c=red i= std1mjt l=1 w=2 mode=include r=1; symbol2 c=blue i= std1mjt l=2 w=2 mode=include r=1; title h=3 \u0026quot;Average root size by group with standard errors of means\u0026quot;; run; quit; Output the plot  ods graphics on / reset=all imagefmt=tiff imagename=\u0026quot;KM_Stented_vs_Stentless\u0026quot;; ods listing style=Statistical gpath=\u0026quot;C:\\xx\u0026quot;; ods graphics on / width=5in; ods listing image_dpi=500;run; To model the change of aorta root over time, time is modeled as a continuous variable in the model. The Class variable time is used in the repeated statement with compound symmetry as the covariance matrix to account for residual correlation within a subject. Mixed effect model is preferred when the longitudinal data is unbalanced. Repeated ANOVA is only used for balanced data. The interaction between group effect and time is tested first, if not significant, will be removed from the model.  **model; *timcl2 is class variable that is placed at the repeated statement to account for patient level (sBAV_ID) cluster; * time is modeled as a countinous variable. proc sort data=rootdat; by sBAV_ID_Number timecl;run; data rootdat; set rootdat; timecl2=time;run; Proc mixed data=rootdat method=ml covtest empirical; class sBAV_ID_Number Root_group (ref=\u0026#39;0\u0026#39;) timecl2; model root_size = time Root_group time*Root_group/ solution; repeated timecl2 / type =CS subject = sBAV_ID_Number; run; ******if the interaction is not significant****; Proc mixed data=rootdat method=ml covtest empirical; class sBAV_ID_Number Root_group (ref=\u0026#39;0\u0026#39;) timecl2; model root_size = time Root_group / solution; repeated timecl2 / type =CS subject = sBAV_ID_Number; run; ******if there were an interaction****; Proc mixed data=rootdat_t method=ml covtest empirical; class sBAV_ID_Number Root_group (ref=\u0026#39;0\u0026#39;) timecl ; model root_size = time2 Root_group time2*Root_group / solution; repeated timecl/ type =CS subject = sBAV_ID_Number; estimate \u0026quot;root group_diff at 5 y\u0026quot; Root_group 1 -1 time2*Root_group 5 -5; estimate \u0026quot;root group_diff at 10 y\u0026quot; Root_group 1 -1 time2*Root_group 10 -10; estimate \u0026quot;root group_diff at 15 y\u0026quot; Root_group 1 -1 time2*Root_group 15 -15; estimate \u0026quot;time effect for group 1\u0026quot; time2 1 0 time2*Root_group 1 0; estimate \u0026quot;time effect for group 0\u0026quot; time2 1 0 time2*Root_group 0 1; run;  "},{"id":11,"href":"/posts/longitudinal-plot/longitudinal-plot/","title":"longitudinal plot (SAS)","section":"Posts","content":" About the data example: The aortic root sizes were measured over time for two groups of patients. The change of root size over time is of interest. In this analysis, we first plotted the group mean at each discrete time to visualize the root growth, secondly modeled the continuous time effect of root growth. The data is setup in a long format that each subject with ID (sBAV_ID_Number) has multiple rows with multiple measurements at different times.\nTo generate longitudinal plots:  *plot the group mean over time with formatting; goptions reset=all ftext=swiss ; axis1 label=(h=2 a=90 \u0026quot;Root size (mm)\u0026quot;) value=( h=2 ); axis2 label= (h=2 \u0026quot;Time (year)\u0026quot;) value=( h=2 ); legend1 label=(f=swiss h=2 \u0026#39;Root Group\u0026#39;) value=(h=2 color=black \u0026#39;root control\u0026#39; \u0026#39;root dilation\u0026#39; ) ; * time2 is a decrete time variable in the dataset; proc sort data=rootdat out=rootdat_t nodupkey; by sBAV_ID_Number time2;run; proc gplot data=rootdat_t; plot root_size*time2=Root_group / haxis=axis2 vaxis=axis1 legend = legend1 ; symbol1 c=red i= std1mjt l=1 w=2 mode=include r=1; symbol2 c=blue i= std1mjt l=2 w=2 mode=include r=1; title h=3 \u0026quot;Average root size by group with standard errors of means\u0026quot;; run; quit; Output the plot  ods graphics on / reset=all imagefmt=tiff imagename=\u0026quot;KM_Stented_vs_Stentless\u0026quot;; ods listing style=Statistical gpath=\u0026quot;C:\\xx\u0026quot;; ods graphics on / width=5in; ods listing image_dpi=500;run; To model the change of aorta root over time, time is modeled as a continuous variable in the model. The Class variable time is used in the repeated statement with compound symmetry as the covariance matrix to account for residual correlation within a subject. Mixed effect model is preferred when the longitudinal data is unbalanced. Repeated ANOVA is only used for balanced data. The interaction between group effect and time is tested first, if not significant, will be removed from the model.  **model; *timcl2 is class variable that is placed at the repeated statement to account for patient level (sBAV_ID) cluster; * time is modeled as a countinous variable. proc sort data=rootdat; by sBAV_ID_Number timecl;run; data rootdat; set rootdat; timecl2=time;run; Proc mixed data=rootdat method=ml covtest empirical; class sBAV_ID_Number Root_group (ref=\u0026#39;0\u0026#39;) timecl2; model root_size = time Root_group time*Root_group/ solution; repeated timecl2 / type =CS subject = sBAV_ID_Number; run; ******if the interaction is not significant****; Proc mixed data=rootdat method=ml covtest empirical; class sBAV_ID_Number Root_group (ref=\u0026#39;0\u0026#39;) timecl2; model root_size = time Root_group / solution; repeated timecl2 / type =CS subject = sBAV_ID_Number; run; ******if there were an interaction****; Proc mixed data=rootdat_t method=ml covtest empirical; class sBAV_ID_Number Root_group (ref=\u0026#39;0\u0026#39;) timecl ; model root_size = time2 Root_group time2*Root_group / solution; repeated timecl/ type =CS subject = sBAV_ID_Number; estimate \u0026quot;root group_diff at 5 y\u0026quot; Root_group 1 -1 time2*Root_group 5 -5; estimate \u0026quot;root group_diff at 10 y\u0026quot; Root_group 1 -1 time2*Root_group 10 -10; estimate \u0026quot;root group_diff at 15 y\u0026quot; Root_group 1 -1 time2*Root_group 15 -15; estimate \u0026quot;time effect for group 1\u0026quot; time2 1 0 time2*Root_group 1 0; estimate \u0026quot;time effect for group 0\u0026quot; time2 1 0 time2*Root_group 0 1; run; "},{"id":12,"href":"/docs/odds-ratio-forest-plot/odds-ratio-forest-plot/","title":"Odds Ratio Forest Plot (R)","section":"Docs","content":" Odds Ratio Forest Plot Odds ratios from logistic models are often present using forest plots. Given the coefficients of logistic models are exponentiated to obtain the odds ratios, odds ratios are NOT symmetric on a linear scale. For example,a change from an odds ratio between 0.4 and 0.5 is the same relative change (25% increase) as that between 2.0 and 2.5. But on the linear scale the difference is 0.1 and 0.5 respectively. An odds ratio between 0-1 indicates a decrease of odds, while an odds ratio greater than 1 indicates an increase in odds. The standard errors of odds ratio will also be misleading when plotting on a linear scale. Although looking at an x-axis in log scale may not be intuitive, it’s a much fair visualization on a log scale when you may compare odds ratio across different risk factors in the forest plot.\nlibrary(ggplot2) # Create labels boxLabels = c(\u0026quot;year\u0026quot;, \u0026quot;age X10\u0026quot;, \u0026quot;hct\u0026quot;, \u0026quot;race (black vs. caucasion)\u0026quot;, \u0026quot;bsa (1.6-1.8 vs. \u0026lt;1.6)\u0026quot;, \u0026quot;bsa (1.8-2.0 vs. \u0026lt;1.6)\u0026quot;, \u0026quot;bsa (\u0026gt;=2.0 vs. \u0026lt;1.6)\u0026quot;, \u0026quot;Gender (female vs. male)\u0026quot;, \u0026quot;diabetes\u0026quot;, \u0026quot;PVD\u0026quot;, ) # you may enter odds ratio and 95% CL either option1:manually or options2: reading in data; # option1: Enter summary data. boxOdds are the odds ratios (calculated elsewhere), boxCILow is the lower bound of the CI, boxCIHigh is the upper bound. df \u0026lt;- data.frame( yaxis = length(boxLabels):1, boxOdds = c(1.012,1.252,0.954,1.497,0.826,0.882,1.164,1.07), boxCILow = c(0.966,1.181,0.943,1.258,0.619,0.666,0.871,0.941), boxCIHigh = c(1.06,1.327,0.965,1.781,1.103,1.167,1.555,1.217) ) ## option2: insert the odds ratio data; df=read.csv(\u0026quot;S:/CardiacSurg/Restricted/MSTCVS/Ting MSTCVS/race aki/oddsratio plot.csv\u0026quot;,header=T,sep=\u0026#39;,\u0026#39;) df$yaxis=length(df$boxLabels):1 # Plot p \u0026lt;- ggplot(df, aes(x = boxOdds, y = yaxis)) # using geom_vline to add reference line for odds ratio=1; p + geom_vline(aes(xintercept = 1), size = .25, linetype = \u0026quot;dashed\u0026quot;) + geom_errorbarh(aes(xmax = boxCIHigh, xmin = boxCILow), size = .5, height = .2, color = \u0026quot;gray50\u0026quot;) + geom_point(size = 3.5, color = \u0026quot;orange\u0026quot;) + theme_bw() + theme(panel.grid.minor = element_blank()) + scale_y_continuous(breaks = df$yaxis, labels = df$boxLabels) + scale_x_continuous(breaks = seq(0,7,0.5) ) + #using log scale for the x axis coord_trans(x = \u0026quot;log10\u0026quot;) + ylab(\u0026quot;\u0026quot;) + xlab(\u0026quot;Odds ratio (log scale)\u0026quot;) + annotate(geom = \u0026quot;text\u0026quot;, y =1.1, x = 3, label =\u0026quot;\u0026quot;, size = 3.5, hjust = 0) + ggtitle(\u0026quot;Odds ratios for AKI stage 2-3\u0026quot;)  "},{"id":13,"href":"/posts/odds-ratio-forest-plot/odds-ratio-forest-plot/","title":"Odds Ratio Forest Plot (R)","section":"Posts","content":" Odds ratios from logistic models are often present using forest plots. Given the coefficients of logistic models are exponentiated to obtain the odds ratios, odds ratios are NOT symmetric on a linear scale. For example,a change from an odds ratio between 0.4 and 0.5 is the same relative change (25% increase) as that between 2.0 and 2.5. But on the linear scale the difference is 0.1 and 0.5 respectively. An odds ratio between 0-1 indicates a decrease of odds, while an odds ratio greater than 1 indicates an increase in odds. The standard errors of odds ratio will also be misleading when plotting on a linear scale. Although looking at an x-axis in log scale may not be intuitive, it’s a much fair visualization on a log scale when you may compare odds ratio across different risk factors in the forest plot.\nlibrary(ggplot2) # Create labels boxLabels = c(\u0026quot;year\u0026quot;, \u0026quot;age X10\u0026quot;, \u0026quot;hct\u0026quot;, \u0026quot;race (black vs. caucasion)\u0026quot;, \u0026quot;bsa (1.6-1.8 vs. \u0026lt;1.6)\u0026quot;, \u0026quot;bsa (1.8-2.0 vs. \u0026lt;1.6)\u0026quot;, \u0026quot;bsa (\u0026gt;=2.0 vs. \u0026lt;1.6)\u0026quot;, \u0026quot;Gender (female vs. male)\u0026quot;, \u0026quot;diabetes\u0026quot;, \u0026quot;PVD\u0026quot;, ) # you may enter odds ratio and 95% CL either option1:manually or options2: reading in data; # option1: Enter summary data. boxOdds are the odds ratios (calculated elsewhere), boxCILow is the lower bound of the CI, boxCIHigh is the upper bound. df \u0026lt;- data.frame( yaxis = length(boxLabels):1, boxOdds = c(1.012,1.252,0.954,1.497,0.826,0.882,1.164,1.07), boxCILow = c(0.966,1.181,0.943,1.258,0.619,0.666,0.871,0.941), boxCIHigh = c(1.06,1.327,0.965,1.781,1.103,1.167,1.555,1.217) ) ## option2: insert the odds ratio data; df=read.csv(\u0026quot;S:/CardiacSurg/Restricted/MSTCVS/Ting MSTCVS/race aki/oddsratio plot.csv\u0026quot;,header=T,sep=\u0026#39;,\u0026#39;) df$yaxis=length(df$boxLabels):1 # Plot p \u0026lt;- ggplot(df, aes(x = boxOdds, y = yaxis)) # using geom_vline to add reference line for odds ratio=1; p + geom_vline(aes(xintercept = 1), size = .25, linetype = \u0026quot;dashed\u0026quot;) + geom_errorbarh(aes(xmax = boxCIHigh, xmin = boxCILow), size = .5, height = .2, color = \u0026quot;gray50\u0026quot;) + geom_point(size = 3.5, color = \u0026quot;orange\u0026quot;) + theme_bw() + theme(panel.grid.minor = element_blank()) + scale_y_continuous(breaks = df$yaxis, labels = df$boxLabels) + scale_x_continuous(breaks = seq(0,7,0.5) ) + #using log scale for the x axis coord_trans(x = \u0026quot;log10\u0026quot;) + ylab(\u0026quot;\u0026quot;) + xlab(\u0026quot;Odds ratio (log scale)\u0026quot;) + annotate(geom = \u0026quot;text\u0026quot;, y =1.1, x = 3, label =\u0026quot;\u0026quot;, size = 3.5, hjust = 0) + ggtitle(\u0026quot;Odds ratios for AKI stage 2-3\u0026quot;) "},{"id":14,"href":"/docs/power-linear-regression-model/simulation-to-calculate-power-in-linear-regression-model/","title":"Use simulation to calculate power in linear regression model (R)","section":"Docs","content":" Use simulation to calculate power in linear regression model Simple power calculations can be done through PROC POWER in SAS. Simulation methods can provide more flexibility in calculating the power of estimates of interest. what we know is the estimated effect size from pilot analysis based on existing data or prior literature effect size. To vary the effect size given we never know the true effect size, we may calculate the statistical powers for increasing or decreasing 5% of the “estimated” effect size. Be cautious not to calculate post hoc power, it’s not meaningful to calculate statistical powers for an effect with a known p value.\n#### use similation to calculate the power in linear regression model############### library(\u0026#39;plyr\u0026#39;) library(\u0026#39;ggplot2\u0026#39;) # the simulation function; regression_sim \u0026lt;- function(simNum, n, b0, b1, b2, b3,b4,b5, b6, b7, b8,b9,b10, err_mean=0, err_sd=74.45294) { # simulate the dummy variables for hospital admission acuity: status1=elective, status2=urgent; status3=emergent; status1 \u0026lt;- sample( 0:1, n, replace=TRUE, prob=c(0.27, 0.73) ) status2 \u0026lt;- sample( 0:1, n, replace=TRUE, prob=c(0.95, 0.05) ) status3 \u0026lt;- sample( 0:1, n, replace=TRUE, prob=c(0.79, 0.21) ) # simulate the dummy variables for euroscore risk score strata euroscore3c1 \u0026lt;- sample( 0:1, n, replace=TRUE, prob=c(0.67, 0.33) ) euroscore3c2 \u0026lt;- sample( 0:1, n, replace=TRUE, prob=c(0.67, 0.33) ) euroscore3c3 \u0026lt;- sample( 0:1, n, replace=TRUE, prob=c(0.67, 0.33) ) # simulate the continous variable team familiairty with estimated mean and std; fourfam \u0026lt;- rnorm(n, mean=23.8, sd=12.1) # the model # generate the outcome y based on the equation; y \u0026lt;- b0 + (b1 * fourfam) + (b2 * status1)+(b3 * status2)+(b4* status3)+ (b5 * euroscore3c1)+(b6 * euroscore3c2)+(b7* euroscore3c3)+ (b8 *fourfam* euroscore3c1)+b9 *fourfam* euroscore3c2+b10 *fourfam* euroscore3c3+ +rnorm(n, mean=err_mean, sd=err_sd) # fun the model with simulation data; model \u0026lt;- lm(y ~ fourfam +status1+status2+status3+euroscore3c1+euroscore3c2+euroscore3c3+ fourfam* euroscore3c1+fourfam* euroscore3c2+fourfam* euroscore3c3) output \u0026lt;- summary(model)$coefficients # identify the coefficients for the interaction terms; # look at the power of detecting team familiarity effect on euroscore strata 1, which will be the sum of #the coefficients of team familairty and the interaction term for team familiarity and euroscore strata 1. # use the delta methods to calculate the variance variance=vcov(model)[2,2]+vcov(model)[9,9]+2*vcov(model)[2,9] # build the wald test; stat= (output[2, 1]+output[9, 1]-0 )/sqrt(variance) # obtain p value; p_values\u0026lt;- 2*pnorm(-abs(stat)) return(p_values) } regression_sim num_sims \u0026lt;- 1000 # repeat the function for 1000 simulation for sample size=1000; # the values for coefficients: b0-b10, the mean and std errors are obtained based on a pilot study on existing data. # to obtain a certain effect, one may vary the effect by increasing or decreasing 5% of the estimated effect. sims \u0026lt;- ldply(1:num_sims, regression_sim, n=1000, b0=226.0049100, b1=-2.5294683, b2=0.0000000, b3=-2.5636306, b4=36.0413179,b5=-78.3913251, b6=-42.9765322, b7=0.0000000, b8=1.5820389, b9=1.2321387, b10=0.0000000, err_mean=0, err_sd=74.45294) # calculate the power. The probability of rejecting the null hypothesis given the alternative hypothesis is true power \u0026lt;- sum(sims \u0026lt; .05) / nrow(sims) # calculate power for various sample sizes; sample_sizes \u0026lt;- c(1000,2000, 3000, 4000, 5000) results \u0026lt;- NULL for (val in sample_sizes) { sims \u0026lt;- ldply(1:100, regression_sim, n=val, b0=226.0049100, b1=-2.5294683, b2=0.0000000, b3=-2.5636306, b4=36.0413179,b5=-78.3913251, b6=-42.9765322, b7=0.0000000, b8=1.5820389, b9=1.2321387, b10=0.0000000, err_mean=0, err_sd=74.45294) sims$n \u0026lt;- val # add the sample size in as a separate column to our results results \u0026lt;- rbind(results, sims) } # conflict between plyr and dplyr. detach plyr, and use library dplyr detach(\u0026quot;package:plyr\u0026quot;, unload=TRUE) library(\u0026#39;dplyr\u0026#39;) # summary the power for various sample size; power_ests \u0026lt;- results %\u0026gt;% group_by(n) %\u0026gt;% summarize(power=sum(V1 \u0026lt; .05) /length(n)) # plot the power with different sample size; ggplot(power_ests, aes(x=n, y=power)) + geom_point() + geom_line() + ylim(c(0, 1)) + theme_minimal()  "},{"id":15,"href":"/posts/power-linear-regression-model/simulation-to-calculate-power-in-linear-regression-model/","title":"Use simulation to calculate power in linear regression model (R)","section":"Posts","content":" Simple power calculations can be done through PROC POWER in SAS. Simulation methods can provide more flexibility in calculating the power of estimates of interest. what we know is the estimated effect size from pilot analysis based on existing data or prior literature effect size. To vary the effect size given we never know the true effect size, we may calculate the statistical powers for increasing or decreasing 5% of the “estimated” effect size. Be cautious not to calculate post hoc power, it’s not meaningful to calculate statistical powers for an effect with a known p value.\n#### use similation to calculate the power in linear regression model############### library(\u0026#39;plyr\u0026#39;) library(\u0026#39;ggplot2\u0026#39;) # the simulation function; regression_sim \u0026lt;- function(simNum, n, b0, b1, b2, b3,b4,b5, b6, b7, b8,b9,b10, err_mean=0, err_sd=74.45294) { # simulate the dummy variables for hospital admission acuity: status1=elective, status2=urgent; status3=emergent; status1 \u0026lt;- sample( 0:1, n, replace=TRUE, prob=c(0.27, 0.73) ) status2 \u0026lt;- sample( 0:1, n, replace=TRUE, prob=c(0.95, 0.05) ) status3 \u0026lt;- sample( 0:1, n, replace=TRUE, prob=c(0.79, 0.21) ) # simulate the dummy variables for euroscore risk score strata euroscore3c1 \u0026lt;- sample( 0:1, n, replace=TRUE, prob=c(0.67, 0.33) ) euroscore3c2 \u0026lt;- sample( 0:1, n, replace=TRUE, prob=c(0.67, 0.33) ) euroscore3c3 \u0026lt;- sample( 0:1, n, replace=TRUE, prob=c(0.67, 0.33) ) # simulate the continous variable team familiairty with estimated mean and std; fourfam \u0026lt;- rnorm(n, mean=23.8, sd=12.1) # the model # generate the outcome y based on the equation; y \u0026lt;- b0 + (b1 * fourfam) + (b2 * status1)+(b3 * status2)+(b4* status3)+ (b5 * euroscore3c1)+(b6 * euroscore3c2)+(b7* euroscore3c3)+ (b8 *fourfam* euroscore3c1)+b9 *fourfam* euroscore3c2+b10 *fourfam* euroscore3c3+ +rnorm(n, mean=err_mean, sd=err_sd) # fun the model with simulation data; model \u0026lt;- lm(y ~ fourfam +status1+status2+status3+euroscore3c1+euroscore3c2+euroscore3c3+ fourfam* euroscore3c1+fourfam* euroscore3c2+fourfam* euroscore3c3) output \u0026lt;- summary(model)$coefficients # identify the coefficients for the interaction terms; # look at the power of detecting team familiarity effect on euroscore strata 1, which will be the sum of #the coefficients of team familairty and the interaction term for team familiarity and euroscore strata 1. # use the delta methods to calculate the variance variance=vcov(model)[2,2]+vcov(model)[9,9]+2*vcov(model)[2,9] # build the wald test; stat= (output[2, 1]+output[9, 1]-0 )/sqrt(variance) # obtain p value; p_values\u0026lt;- 2*pnorm(-abs(stat)) return(p_values) } regression_sim num_sims \u0026lt;- 1000 # repeat the function for 1000 simulation for sample size=1000; # the values for coefficients: b0-b10, the mean and std errors are obtained based on a pilot study on existing data. # to obtain a certain effect, one may vary the effect by increasing or decreasing 5% of the estimated effect. sims \u0026lt;- ldply(1:num_sims, regression_sim, n=1000, b0=226.0049100, b1=-2.5294683, b2=0.0000000, b3=-2.5636306, b4=36.0413179,b5=-78.3913251, b6=-42.9765322, b7=0.0000000, b8=1.5820389, b9=1.2321387, b10=0.0000000, err_mean=0, err_sd=74.45294) # calculate the power. The probability of rejecting the null hypothesis given the alternative hypothesis is true power \u0026lt;- sum(sims \u0026lt; .05) / nrow(sims) # calculate power for various sample sizes; sample_sizes \u0026lt;- c(1000,2000, 3000, 4000, 5000) results \u0026lt;- NULL for (val in sample_sizes) { sims \u0026lt;- ldply(1:100, regression_sim, n=val, b0=226.0049100, b1=-2.5294683, b2=0.0000000, b3=-2.5636306, b4=36.0413179,b5=-78.3913251, b6=-42.9765322, b7=0.0000000, b8=1.5820389, b9=1.2321387, b10=0.0000000, err_mean=0, err_sd=74.45294) sims$n \u0026lt;- val # add the sample size in as a separate column to our results results \u0026lt;- rbind(results, sims) } # conflict between plyr and dplyr. detach plyr, and use library dplyr detach(\u0026quot;package:plyr\u0026quot;, unload=TRUE) library(\u0026#39;dplyr\u0026#39;) # summary the power for various sample size; power_ests \u0026lt;- results %\u0026gt;% group_by(n) %\u0026gt;% summarize(power=sum(V1 \u0026lt; .05) /length(n)) # plot the power with different sample size; ggplot(power_ests, aes(x=n, y=power)) + geom_point() + geom_line() + ylim(c(0, 1)) + theme_minimal() "},{"id":16,"href":"/posts/blog-down/blog-down/","title":"Using R Blogdown for Site Building","section":"Posts","content":"It\u0026rsquo;s such a learning experience in setting up a website using github+ terminal + R blogdown + netlify for me. In general, I followed the post from [Alison Hill] (https://alison.rbind.io/post/up-and-running-with-blogdown/).\nA couple of problems have occured and I have searched very hard to resolve them.\n   R crash whenever I tried to open projects in Mac OS system.    Solution: download update R version from (https://www.rstudio.com/products/rstudio/download/). Now I have Rstudio 1.1.463 - Mac OS X in my mac.\n  I don\u0026rsquo;t know how to push the local materials to github.    Solution: It seems there are several ways to push local files to github. In R I follow the path Tools \u0026gt; Version Control \u0026gt; git. Then I select the folders that I would like to upload to github. However, some of the folders were NOT staged. In another word, I could not select these Not staged files in order to commit and push them to github.\nTo go around this, I used terminal. In terminal, I used cd to go to the local github clone file. Then I used command git add -A . All files in the folders were able to upload to github then.\n  I could not publish my sites in netlify.    After uploading all neccessary needed files into github, especially the \u0026lsquo;public\u0026rsquo; folder, I followed the deploy setting but failed. The key step here turn out to be that you need to make sure that the hugo versions are consistent in your machine and netlify.\nIn terminal, I typed in `` Hugo versoins` to obtain the hugo versions 0.51 in my machine. In netlify, I added the variables in the deploy setting, Hugo_version, and set the value of 0.51.\n  Create a new post option 1: use addins \u0026lt; NewPost\u0026gt; which will create a newpost under the folder Content/Post option 2: create a Post folder under Content/Post    Insert an image in RMD file and show it in blogdown    option 1: use addins during the edits of .md file This will generate a code in the file for example\n![](/post/2019-03-27-median-survival-time_files/survivalpost1.png) The file directory is under static/ with the relative directory path\noption 2: put the file in the corresponding post file under static/ , and enter the file relative file path. For example, here is another code to add in a image which will allow image size adjustment\nknitr::include_graphics(\u0026quot;/post/2019-03-27-median-survival-time_files/survivalpost2.png\u0026quot;)   use git shell language to push changes Reference: [link] (https://help.github.com/en/articles/adding-an-existing-project-to-github-using-the-command-line)    I did three lines of commands git add . git commit -m \u0026ldquo;change\u0026rdquo; git push -u origin master\n  I was trying to change my themes from academic to a more simple documentation theme. Instead of doing install_theme () which will create many errors during the installation. You may follow the direction https://geekdocs.de/usage/getting-started/ . step1: you may first create a new site in the git clone folder, using new_site () in R, which will use a default theme. step2: install the geekdoc theme in the site folder. You should see geekdoc folder appear in theme folder within the site folder. step 3: you may just change the config.toml to the theme of geekdoc. Then run serve_site().    "},{"id":17,"href":"/docs/psm/ps-weighting/ps-weighting/","title":"2.Propensity score weighting methods (R)","section":"Propensity score methods","content":" Propensity score weighting methods (R) Propensity score (PS) weighting methods include inverse probability treatment weighting (IPTW). , matching weighting, overlap weighting. These methods all used propensity score to weight the observations, resulting in a “pseudo” balancing sample.\nInverse probability weighting is a common approach, however, often come with a problem with extreme weights. In the case of extreme weights, some cases may be weighted much larger than the other cases, thus skew the effect. To deal with extreme weight, one may suggest truncating the weight. But this will exclude some observations, and the cutoff of the “extreme” weight is often artificial. In addition, when the propensity scores of the treatment and control groups have little area of overlapping, IPTW violates the assumption of common support. Best practice for IPTW can be reviewed in this article (Ref: PC. Austin Stat Med 2015). The alternative to IPTW is to use overlapping weight. Here is the key reference for overlapping weight: https://www2.stat.duke.edu/~fl35/OW.html\nHere is the key point about the relationship between overlapping weight vs. regular IPTW weight. In below, h(x) will be the numerator, and the regular weight (denominator) would be the inverse probability weight. Applying a different numerator h(x) will yield different weights (weight (w1,w0)).\n Fig.1 (Source: Fan Li https://www2.stat.duke.edu/~fl35/OW.html)\n Fig.2 ((Source: Fan Li https://www2.stat.duke.edu/~fl35/OW.html))\nHere the $ {w}_{i} $ (weight) can be replaced by different weights in Fig 1.\nTo obtain the “double robust” estimator (i.e. the augmented estimator), we could follow this equation:\n Fig. 3 (Source: Liang Li* and Tom Greene. A Weighting Analogue to Pair Matching in Propensity Score Analysis. The international Journal of Biostatistics 2013)\nAfter applying the weights, it’s critical to check balance of the weighted sample. Standardized mean difference can be calculated and ploted.\nHere are the R codes to apply the weights and calculate the IPTW ATE or the overlapping weight ATE.\n### step 1. propensity score model#### ps_model \u0026lt;- glm(extub_or2 ~ age+ gender_+ race_ + chrlungd_ +nyh34+ hdef_4c +status_ +robotic_ +armusproctype+centerid ,data=dat3,family = binomial(link = \u0026quot;logit\u0026quot;) ) summary (ps_model) dat3$hatpi=predict(ps_model, dat3, type = \u0026quot;response\u0026quot;) # calculate the propensity score (the probability of having the treatment) ### step2. outcome model### outcome_model \u0026lt;-glm(icuinhrs ~extub_or2+ age +gender_ +race_ +chrlungd_ +classnyh+ hdef_4c+ status_+ robotic_ +armusproctype+centerid, data=dat3) summary (outcome_model) library(dplyr) # set dataset with treatment X=1 or X=0; modeldat1\u0026lt;- mutate(dat3, extub_or2 = replace(extub_or2, extub_or2==0, 1)) modeldat0\u0026lt;- mutate(dat3, extub_or2 = replace(extub_or2,extub_or2==1, 0)) # calculate the predicted Y (the expected outcome) given X=1 or X=0; dat3$predicty1=predict(outcome_model, modeldat1, type = \u0026quot;response\u0026quot;) dat3$predicty0=predict(outcome_model, modeldat0, type = \u0026quot;response\u0026quot;) #### Step3. ATE or augmented ATE based on propensity score weighting### dat3$A=dat3$extub_or2 #the treatment dat3$Y=dat3$icuinhrs # the observed outcome #overlap weight dat3$h=(1-dat3$hatpi)*dat3$hatpi #the h \u0026quot;numerator for overlap weight dat3$w_weight =dat3$h/ (dat3$A*dat3$hatpi + (1-dat3$A)*(1-dat3$hatpi)) #the overlap weight- apply the numerators #matching weight dat3$h2=pmin((1-dat3$hatpi), dat3$hatpi) #the h \u0026quot;numerator for matching weight dat3$w_weight2 =dat3$h2/ (dat3$A*dat3$hatpi + (1-dat3$A)*(1-dat3$hatpi)) #the matching weight #ATE with overlap weight ate= sum(dat3$A*dat3$w_weight*dat3$Y)/sum(dat3$A*dat3$w_weight)- sum((1-dat3$A)*dat3$w_weight*dat3$Y)/sum((1-dat3$A)*dat3$w_weight) #Augmented ATE with overlap weight aug_ate= sum (dat3$w_weight*(dat3$predicty1-dat3$predicty0))/sum(dat3$w_weight)+ sum (dat3$A*dat3$w_weight*(dat3$Y-dat3$predicty1))/sum(dat3$w_weight*dat3$A)- sum ((1-dat3$A)*dat3$w_weight*(dat3$Y-dat3$predicty0))/sum(dat3$w_weight*(1-dat3$A)) #ATE with matching weight ate2= sum(dat3$A*dat3$w_weight2*dat3$Y)/sum(dat3$A*dat3$w_weight2)- sum((1-dat3$A)*dat3$w_weight2*dat3$Y)/sum((1-dat3$A)*dat3$w_weight2) #Augmented ATE with matching weight +augmentation aug_ate2= sum (dat3$w_weight2*(dat3$predicty1-dat3$predicty0))/sum(dat3$w_weight2)+sum (dat3$A*dat3$w_weight2*(dat3$Y-dat3$predicty1))/sum(dat3$w_weight2*dat3$A)-sum ((1-dat3$A)*dat3$w_weight2*(dat3$Y-dat3$predicty0))/sum(dat3$w_weight2*(1-dat3$A)) #### assess balance of propensity weighting samples ############ ## Weighted analysis library(survey) library(tableone) ## Reorganizing data library(reshape2) ## plotting library(ggplot2) vars\u0026lt;- c(\u0026quot;age\u0026quot;, \u0026quot;gender_\u0026quot;, \u0026quot;race_\u0026quot;, \u0026quot;chrlungd_\u0026quot;,\u0026quot;nyh34\u0026quot;, \u0026quot;hdef_4c\u0026quot;,\u0026quot;status_\u0026quot;,\u0026quot;robotic_\u0026quot;,\u0026quot;armusproctype\u0026quot;,\u0026quot;centerid\u0026quot;) catvar\u0026lt;-c(\u0026quot;gender_\u0026quot;, \u0026quot;race_\u0026quot;, \u0026quot;chrlungd_\u0026quot;,\u0026quot;nyh34\u0026quot;, \u0026quot;hdef_4c\u0026quot;,\u0026quot;status_\u0026quot;,\u0026quot;robotic_\u0026quot;,\u0026quot;armusproctype\u0026quot;,\u0026quot;centerid\u0026quot;) tabUnmatched \u0026lt;- CreateTableOne(vars = vars, strata = \u0026quot;extub_or2\u0026quot;, data = dat3,factorVars = catvar, test = FALSE) print(tabUnmatched, smd = TRUE) # overlap weighted data datOw \u0026lt;- svydesign(ids = ~ 1, data = dat3, weights = ~ w_weight) tabWeightedOw \u0026lt;- svyCreateTableOne(vars = vars, strata = \u0026quot;extub_or2\u0026quot;, data = datOw, factorVars = catvar,test = FALSE) ## Show table with SMD print(tabWeightedOw, smd = TRUE) # matching weighted data datMatch \u0026lt;- svydesign(ids = ~ 1, data = dat3, weights = ~ w_weight2) tabMatched \u0026lt;- svyCreateTableOne(vars = vars, strata = \u0026quot;extub_or2\u0026quot;, data = datMatch, factorVars = catvar,test = FALSE) ## Show table with SMD print(tabMatched, smd = TRUE) ## Construct a data frame containing variable name and SMD from all methods dataPlot \u0026lt;- data.frame(variable = rownames(ExtractSmd(tabUnmatched)), Unmatched = as.numeric(ExtractSmd(tabUnmatched)), Matched = as.numeric(ExtractSmd(tabMatched)), # Weighted = as.numeric(ExtractSmd(tabWeighted)), WeightedOw = as.numeric(ExtractSmd(tabWeightedOw))) ## Create long-format data for ggplot2 dataPlotMelt \u0026lt;- melt(data = dataPlot, id.vars = c(\u0026quot;variable\u0026quot;), variable.name = \u0026quot;Method\u0026quot;, value.name = \u0026quot;SMD\u0026quot;) ## Order variable names by magnitude of SMD varNames \u0026lt;- as.character(dataPlot$variable)[order(dataPlot$Unmatched)] ## Order factor levels in the same order dataPlotMelt$variable \u0026lt;- factor(dataPlotMelt$variable, levels = varNames) ## Plot the SMD using ggplot2 ggplot(data = dataPlotMelt, mapping = aes(x = variable, y = SMD, group = Method, color = Method)) + geom_line() + geom_point() + geom_hline(yintercept = 0.1, color = \u0026quot;black\u0026quot;, size = 0.1) + coord_flip() + theme_bw() + theme(legend.key = element_blank()) ## present the SMD using table ## Column bind tables resCombo \u0026lt;- cbind(print(tabUnmatched, printToggle = FALSE), print(tabMatched, printToggle = FALSE), #print(tabWeighted, printToggle = FALSE), print(tabWeightedOw, printToggle = FALSE)) ## Add group name row, and rewrite column names resCombo \u0026lt;- rbind(Group = rep(c(\u0026quot;treatment\u0026quot;,\u0026quot;control\u0026quot;), 4), resCombo) colnames(resCombo) \u0026lt;- c(\u0026quot;Unmatched\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;MW\u0026quot;,\u0026quot;\u0026quot;\u0026quot;OW\u0026quot;,\u0026quot;\u0026quot;) print(resCombo, quote = FALSE) write.csv(resCombo, file = \u0026quot;.../ps_smdtable.csv\u0026quot;)  "}]